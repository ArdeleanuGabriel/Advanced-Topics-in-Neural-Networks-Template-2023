{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "LHOO0TVCJl2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from multiprocessing import freeze_support\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ],
      "metadata": {
        "id": "D5k5x9JfMqHV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='b3b01019822e0451300a1e9ea1f3310d63834d44')\n",
        "config = {'method': 'random',\n",
        "          'metric': {'goal': 'maximize', 'name': 'acc_val'},\n",
        "          'parameters': {'val_batch_size': {'distribution': 'q_log_uniform_values', 'max': 256, 'min': 32, 'q': 8},\n",
        "                         'epochs': {'value': 50},\n",
        "                         'learning_rate': {'distribution': 'uniform', 'max': 0.01, 'min': 0},\n",
        "                         }\n",
        "          }\n",
        "\n",
        "sweep_id = wandb.sweep(config, project=\"ATNN-Lab05\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKltW7RMr5l",
        "outputId": "f114c596-9191-45ad-88bf-ef83569d98cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: cdffla3s\n",
            "Sweep URL: https://wandb.ai/gardeleanu151/ATNN-Lab05/sweeps/cdffla3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input, hidden, output):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input, hidden)\n",
        "        self.fc2 = torch.nn.Linear(hidden, output)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class cached_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, cache=True):\n",
        "        if cache:\n",
        "            dataset = tuple([x for x in dataset])\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset[i]"
      ],
      "metadata": {
        "id": "TsoKRo9kbnaY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device('mos')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def get_accuracy(results, targets):\n",
        "    fp_fn = torch.logical_not(results == targets).sum().item()\n",
        "    total = len(results)\n",
        "    return (total - fp_fn) / total\n",
        "\n",
        "def train(model, train_loader, criteria, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    all_results = []\n",
        "    all_targets = []\n",
        "    loss = 0\n",
        "    batch_loss = []\n",
        "    for data, targets in train_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        result = model(data)\n",
        "        loss = criteria(result, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        loss += loss.item()\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "        result = result.softmax(dim=1).detach().cpu().squeeze()\n",
        "        targets = targets.cpu().squeeze()\n",
        "        all_results.append(result)\n",
        "        all_targets.append(targets)\n",
        "\n",
        "    all_results = torch.cat(all_results).argmax(dim=1)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "\n",
        "    return round(get_accuracy(all_results, all_targets), 4), loss, batch_loss\n",
        "\n",
        "def validate(model, valid_loader, criteria, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_results = []\n",
        "    all_targets = []\n",
        "\n",
        "    val_loss = 0\n",
        "    for data, targets in valid_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            results = model(data)\n",
        "\n",
        "        results = results.softmax(dim=1).cpu().squeeze()\n",
        "        targets = targets.squeeze()\n",
        "\n",
        "        loss = criteria(results, targets)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        all_results.append(results)\n",
        "        all_targets.append(targets)\n",
        "\n",
        "    all_results = torch.cat(all_results).argmax(dim=1)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "\n",
        "    return round(get_accuracy(all_results, all_targets), 4), val_loss\n",
        "\n",
        "def run_epoch(model, train_loader, valid_loader, criteria, optimizer, device):\n",
        "    acc, loss, batch_loss = train(model, train_loader, criteria, optimizer, device)\n",
        "    valid_acc, valid_loss = validate(model, valid_loader, criteria, device)\n",
        "    return acc, valid_acc, loss, valid_loss, batch_loss\n",
        "\n",
        "def get_norm(model):\n",
        "    norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        norm += torch.norm(param)\n",
        "    return norm\n"
      ],
      "metadata": {
        "id": "t_2NMxDZci5n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(device=get_device()):\n",
        "    transforms = [v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Resize((28, 28), antialias=True), v2.Grayscale(), torch.flatten,]\n",
        "\n",
        "    data_path = '../data'\n",
        "    train_data = CIFAR10(root=data_path, train=True, transform=v2.Compose(transforms), download=True)\n",
        "    valid_data = CIFAR10(root=data_path, train=False, transform=v2.Compose(transforms), download=True)\n",
        "    train_data = cached_dataset(train_data)\n",
        "    valid_data = cached_dataset(valid_data)\n",
        "\n",
        "    model = MLP(784, 100, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    with wandb.init(config=None):\n",
        "        config = wandb.config\n",
        "        epochs = config.epochs\n",
        "        batch_size = 256\n",
        "        val_batch_size = config.val_batch_size\n",
        "        num_workers = 2\n",
        "        persistent_workers = (num_workers != 0)\n",
        "        criteria = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        train_loader = DataLoader(train_data, shuffle=True, pin_memory=pin_memory, num_workers=num_workers, batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
        "        valid_loader = DataLoader(valid_data, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size, drop_last=False)\n",
        "\n",
        "        pin_memory = device.type == 'cuda'\n",
        "\n",
        "        summ_writer = SummaryWriter()\n",
        "        tbar = tqdm(tuple(range(epochs)))\n",
        "\n",
        "        #optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        #optimizer = torch.optim.Adagrad(model.parameters(), lr=config.learning_rate)\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
        "        #optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        for epoch in tbar:\n",
        "            acc, acc_val, loss, valid_loss, batch_loss = run_epoch(model, train_loader, valid_loader, criteria, optimizer, device)\n",
        "            tbar.set_postfix_str(f\"Acc: {acc}, Acc_val: {acc_val}\")\n",
        "\n",
        "            summ_writer.add_scalar(\"Train/Loss\", loss / len(train_loader), epoch)\n",
        "            summ_writer.add_scalar(\"Train/Accuracy\", acc, epoch)\n",
        "\n",
        "            summ_writer.add_scalar(\"Val/Loss\", valid_loss / len(valid_loader), epoch)\n",
        "            summ_writer.add_scalar(\"Val/Accuracy\", acc_val, epoch)\n",
        "\n",
        "            summ_writer.add_scalar(\"Model/Norm\", get_norm(model), epoch)\n",
        "            summ_writer.add_scalar(\"Constants/Learning rate\", config.learning_rate, epoch)\n",
        "            summ_writer.add_scalar(\"Constants/Batch size\", val_batch_size, epoch)\n",
        "\n",
        "            for b, l in enumerate(batch_loss):\n",
        "                summ_writer.add_scalar(\"Batch Train/Loss\", l, b)\n",
        "\n",
        "            wandb.log({\"acc_val\": acc_val, \"epoch\": epoch})\n",
        "\n",
        "    summ_writer.close()\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    wandb.agent(sweep_id, main, count=3)"
      ],
      "metadata": {
        "id": "zn1Pyp3bJAzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}